<div align="justify">

# Registration-guided 3D-UNets for AC-PC Landmark Prediction on CT

[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)
![Patent Pending](https://img.shields.io/badge/Status-Patent%20Pending-blue)
> This software is **Patent Pending** (US App. No. 18/534,822) and is strictly for **Non-Commercial Academic Use**. See the [License & Patent Notice](#license--patent-notice) section below for details.

This repository provides code to develop and evaluate a framework of image registration informed 3D-UNets to automatically localize the Anterior (AC) and Posterior Commissure (PC) landmarks on non-contrast Computed Tomography (CT) scans. 

## Model Availability Disclaimer
As this framework was originally developed using data from the Dept. of Veterans Affairs (VA), we are unable to publicly release the model weights trained on internal VA head CT scans due to patient privacy regulations and data security policies. However, we provide the complete source code and a detailed step-by-step tutorial to enable researchers to train the model using their own datasets.

## Data Requirements
We developed this framework on a structurally diverse set of 400+ non-contrast CT scans from 340 patients with Normal Pressure Hydrocephalus (NPH), Alzheimer's Disease (AD), post-traumatic volume loss (PTVL), and headache (HC) by consecutively including patients to maximize training data size. We also employ rotational augmentations in the training data (emperically determined to be optimal at 14 3D rotations per scan) to improve generalization. As this model generalized to two external datasets, we recommend that researchers aim to include at least ~100 scans per neurological condition, with sufficient data augmentation. 

The AC-PC reference standard that we used as ground-truth to train our models were generated by a team of multiple annotators, guided by a protocol developed by a board-certified neuroradiologist. The landmarks were annotated on the central intercommissural line (CIL). See [Choi et al. 2013](https://pubmed.ncbi.nlm.nih.gov/23901324/) for visualizing the landmarks on the CIL. 

## System and Add-on Requirements
### Coarse Localization
The coarse-localization step for our framework was run fully within 3D Slicer using a Jupyter Notebook interface, which can be found in [CoarseLocalizationSlicer.ipynb](./CoarseLocalizationSlicer.ipynb). We used [3D Slicer version 5.6.2](https://download.slicer.org/?version=5.6.2). The following extensions to 3D Slicer, which can be downloaded from 3D-Slicer (View > Extensions Manager) are required: 
1. [SlicerJupyter](https://github.com/Slicer/SlicerJupyter)
2. [SlicerElastix](https://github.com/lassoan/SlicerElastix)
3. [SurfaceWrapSolidify](https://github.com/sebastianandress/Slicer-SurfaceWrapSolidify)

[SlicerNeuro](https://github.com/Slicer/SlicerNeuro) offers the "ACPC Transform" module to visualize an AC-PC transformed CT scan using AC/PC predictions from our framework. Note that certain modules like the <SwissSkullStripper>, which get automatically installed with SlicerNeuro may interfere with the workflow if they are not removed manually. We will provide a jupyter notebook for visualizing post AC-PC aligned scans shortly. 
 
### Fine Localization
Our 3D-UNet models was trained with data of scale described above on a Linux machine (Ubuntu 24.04.3 LTS) with the following specifications:\
CPU: Intel(R) Core(TM) i9-10850K CPU @ 3.60GHz 20M Cache\
Cores: 10 Cores / 20 Threads\
Memory: 32GB\
GPU: NVIDIA GeForce RTX 3080 (10GB VRAM)

*Note on keeping training times reasonable with large-scale image data: The registration-guided 3D-UNets require 2-channel (for the two landmarks: AC and PC) 3D-patches of image data as input and 6-channel 3D-patches of ground-truth heatmaps (2 landmarks x 3 channels \[AC, PC, background] = 6 operational channels) for supervised training. Our experience with assembling the inputs and outputs on the fly with dataloaders resulted in very long training times, where multiple model versions have to be evaluated for hyperparameter tuning. Therefore, we assemble intermediately processed data which is first fully read into CPU memory. During training, batches of data were fully processed and moved into GPU memory.* 

## Schematic of the Registration-guided 3D-UNet Framework
![System Architecture Diagram](assets/Registration-guided-3D-UNet-Framework.jpg)
Figure 1. Overview of the AC-PC Localization Methodology. Coarse landmarks inferred via image registration (coarse localization), guide selection of image patches which are input to the 3D-UNet to perform heatmap regression around the true AC and PC landmarks, along with a background heatmap. Patch and channel specific predictions are unstacked and normalized, followed by derivation of AC-PC predictions (fine localization) as the mean location of active voxels in the regressed heatmaps. AC = Anterior Commissure, PC = Posterior-Commissure, BG = Background, ps = patch-size, BN = Batch-Normalization, ReLU = Rectified Linear Unit. Note that processing is in 3D, and 2D input slices and heatmaps are indicated for demonstration purposes. 

## Illustration of AC-PC landmark prediction on a sample CT scan 
![AC-PC Illustration](assets/CQ500_CT_185.jpg)
Figure 2. Visualization of AC-PC prediction on a representative head CT scan obtained from the public CQ500 dataset [1]. This 5-mm CT scan was selected based on the accompanying radiology reports indicating no intracranial abnormalities, serving here as a representative example of normal anatomy. The coarse AC-PC landmarks derived via image registration (in *yellow*), manually annotated reference standard landmarks (in *yellow*), and fine localized landmarks from the registration-guided 3D-UNet (in *green*) are shown. Overall AC/PC localization error (mean radial distance between the fine localized and reference standard landmarks) were 1.7/1.25 mm. Note: This volume is used solely for illustrative purposes to demonstrate the reference standard alongside coarse and fine-localized predictions; it was not included in the training, validation, or testing cohorts used in this study. The CQ500 scan is displayed here for demonstration because the clinical datasets used in this study are restricted from public display due to institutional privacy regulations.

## Acknowledgments

This work utilized **3D Slicer** (https://www.slicer.org/) for image preprocessing and obtaining coarse localized AC-PC landmarks via image registration. If you use this code for your research, please consider citing the following (in addition to our paper):
* **3D Slicer:** Fedorov A., Beichel R., Kalpathy-Cramer J., et al. "3D Slicer as an image computing platform for the Quantitative Imaging Network." *Magnetic Resonance Imaging*. 2012 Nov;30(9):1323-41.

## License & Patent Notice

**Copyright:**
This project is licensed under the **Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)**. 

**Patents:**
The methods and algorithms implemented in this software are the subject of **US Patent Application No. 18/534,822**. 

Commercial use of this software, or the methods described herein, is strictly prohibited without a separate commercial license. For commercial licensing inquiries, please contact: uzma@samadani.com.

**Third-Party Components:**
This repository contains code adapted from *pytorch-3dunet* by Adrian Wolny, used under the MIT License. See [model.py](model.py) for details.

**References**
[1] Chilamkurthy, S., Ghosh, R., Tanamala, S., et al. "Deep learning algorithms for detection of critical findings in head CT scans." arXiv preprint arXiv:1803.05854 (2018).

</div>
